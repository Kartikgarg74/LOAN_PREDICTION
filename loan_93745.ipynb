{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279d79bc-01af-4009-8de3-f8f930f9473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fold 0, bag 0: ROC-AUC-Score=0.94884\n",
      "# Fold 0, bag 1: ROC-AUC-Score=0.94943\n",
      "# Fold 0, bag 2: ROC-AUC-Score=0.94879\n",
      "# Fold 1, bag 0: ROC-AUC-Score=0.95041\n",
      "# Fold 1, bag 1: ROC-AUC-Score=0.95075\n",
      "# Fold 1, bag 2: ROC-AUC-Score=0.95045\n",
      "# Fold 2, bag 0: ROC-AUC-Score=0.95259\n",
      "# Fold 2, bag 1: ROC-AUC-Score=0.95528\n",
      "# Fold 2, bag 2: ROC-AUC-Score=0.95313\n",
      "# Fold 3, bag 0: ROC-AUC-Score=0.94946\n",
      "# Fold 3, bag 1: ROC-AUC-Score=0.95204\n",
      "# Fold 3, bag 2: ROC-AUC-Score=0.95247\n",
      "# Fold 4, bag 0: ROC-AUC-Score=0.95462\n",
      "# Fold 4, bag 1: ROC-AUC-Score=0.95565\n",
      "# Fold 4, bag 2: ROC-AUC-Score=0.95456\n",
      "#ROC-AUC mean: 0.9518991 (+- 0.0022951)#   elapsed time:   0 min\n",
      "# Fold 0, bag 0: ROC-AUC-Score=0.94955\n",
      "# Fold 0, bag 1: ROC-AUC-Score=0.94924\n",
      "# Fold 0, bag 2: ROC-AUC-Score=0.94846\n",
      "# Fold 1, bag 0: ROC-AUC-Score=0.95252\n",
      "# Fold 1, bag 1: ROC-AUC-Score=0.95253\n",
      "# Fold 1, bag 2: ROC-AUC-Score=0.95027\n",
      "# Fold 2, bag 0: ROC-AUC-Score=0.95158\n",
      "# Fold 2, bag 1: ROC-AUC-Score=0.95315\n",
      "# Fold 2, bag 2: ROC-AUC-Score=0.95501\n",
      "# Fold 3, bag 0: ROC-AUC-Score=0.94941\n",
      "# Fold 3, bag 1: ROC-AUC-Score=0.95160\n",
      "# Fold 3, bag 2: ROC-AUC-Score=0.95079\n",
      "# Fold 4, bag 0: ROC-AUC-Score=0.95351\n",
      "# Fold 4, bag 1: ROC-AUC-Score=0.95545\n",
      "# Fold 4, bag 2: ROC-AUC-Score=0.95354\n",
      "#ROC-AUC mean: 0.9517755 (+- 0.0020716)#   elapsed time:   3 min\n",
      "Optimal weights: [0.72112682 0.21579582 0.06307737]\n",
      "Best ROC-AUC Score: 0.9577929521214057\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the data\n",
    "test_data = pd.read_csv(\"/Users/raghavgarg/Downloads/playground-series-s4e10/test.csv\")\n",
    "train_data = pd.read_csv(\"/Users/raghavgarg/Downloads/playground-series-s4e10/train.csv\")\n",
    "original_data = pd.read_csv(\"/Users/raghavgarg/Downloads/credit_risk_dataset.csv\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(X):\n",
    "    X = X.copy()\n",
    "    # Creating derived features\n",
    "    X['age_income_interaction'] = X['person_age'] * X['person_income']\n",
    "    X['loan_to_emp_length_ratio'] = X['loan_amnt'] / (X['person_emp_length'] + 1)\n",
    "    monthly_income = X['person_income'] / 12\n",
    "    X['monthly_debt'] = X['loan_amnt'] * (1 + X['loan_int_rate']) / 12\n",
    "    X['dti_ratio'] = X['monthly_debt'] / monthly_income\n",
    "    X['risk_flag'] = np.where((X['cb_person_default_on_file'] == 'Y') & (X['loan_grade'].isin(['C', 'D', 'E'])), 1, 0)\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "    # Creating additional derived features\n",
    "    X['person_income_to_age'] = X['person_income'] / (X['person_age'] + 1)  # to avoid division by zero\n",
    "    X['loan_amnt_to_income_ratio'] = X['loan_amnt'] / (X['person_income'] + 1)  # to avoid division by zero\n",
    "    X['emp_length_to_age_ratio'] = X['person_emp_length'] / (X['person_age'] + 1)  # to avoid division by zero\n",
    "\n",
    "    # Update numerical columns list\n",
    "    numerical_cols += ['person_income_to_age', 'loan_amnt_to_income_ratio', 'emp_length_to_age_ratio']\n",
    "\n",
    "    # Preprocessing for numerical data: Impute and scale\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Preprocessing for categorical data: One-hot encoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the data using the preprocessor\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Get column names for numerical and one-hot encoded categorical features\n",
    "    cat_col_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "    # Combine numerical and categorical column names\n",
    "    all_col_names = numerical_cols + list(cat_col_names)\n",
    "\n",
    "    # Convert the transformed array back to a pandas DataFrame\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=all_col_names)\n",
    "\n",
    "    return X_transformed_df\n",
    "\n",
    "def down_sampling(X, y, i):\n",
    "    \"\"\"Perform down-sampling on the majority class.\"\"\"\n",
    "    # Convert y to a Pandas Series if it's a NumPy array\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    # Reset index to ensure proper alignment\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    majority_class = X[y == 0]\n",
    "    minority_class = X[y == 1]\n",
    "\n",
    "    # Down-sample majority class\n",
    "    majority_sample = majority_class.sample(len(minority_class), random_state=i)\n",
    "\n",
    "    # Combine the down-sampled majority class with the minority class\n",
    "    y_minimal = pd.concat([y.iloc[majority_sample.index], y[y == 1]])\n",
    "\n",
    "    # Ensure X_minimal is a DataFrame\n",
    "    X_minimal = pd.concat([majority_sample, minority_class], axis=0)\n",
    "\n",
    "    return X_minimal, y_minimal\n",
    "    \n",
    "# Define models\n",
    "xgb_model = XGBClassifier(n_estimators=2000, early_stopping_rounds=100, eval_metric='auc', max_bin=262143, n_jobs=4, random_state=0)\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    'n_estimators': 3000,\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 42,\n",
    "    'learning_rate': 0.0322942967545754,\n",
    "    'num_leaves': 24,\n",
    "    'max_depth': 15,\n",
    "    'min_data_in_leaf': 25,\n",
    "    'feature_fraction': 0.6236144085285287,\n",
    "    'bagging_fraction': 0.9596685778433888,\n",
    "    'bagging_freq': 3,\n",
    "    'verbose': -1,\n",
    "}\n",
    "lgbm_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "\n",
    "def cross_validate(n_splits=5, n_bags=3, model_name='xgb'):\n",
    "    \"\"\"Compute out-of-fold and test predictions for a model.\"\"\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    scores = []\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    test_preds = np.zeros(len(test_data))\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=63)\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(X, y)):\n",
    "        for i in range(n_bags):\n",
    "            X_train = X.iloc[train_index]\n",
    "            y_train = y[train_index]  # Use y[train_index] instead of y.iloc[train_index]\n",
    "            X_val = X.iloc[valid_index]\n",
    "            y_val = y[valid_index]  # Use y[valid_index] instead of y.iloc[valid_index]\n",
    "\n",
    "            X_train, y_train = down_sampling(X_train, y_train, 10 * fold + i)\n",
    "\n",
    "            if model_name == 'lgb':\n",
    "                m = clone(lgbm_model)\n",
    "                m.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc')\n",
    "            elif model_name == 'xgb':\n",
    "                m = clone(xgb_model)\n",
    "                m.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "            y_pred = m.predict_proba(X_val)[:, 1]\n",
    "            score = roc_auc_score(y_val, y_pred)\n",
    "            print(f\"# Fold {fold}, bag {i}: ROC-AUC-Score={score:.5f}\")\n",
    "            scores.append(score)\n",
    "            oof_preds[valid_index] += y_pred / n_bags\n",
    "            test_preds += m.predict_proba(test_data)[:, 1] / (kfold.get_n_splits() * n_bags)\n",
    "\n",
    "            del m\n",
    "            gc.collect()\n",
    "\n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    print(f\"#ROC-AUC mean: {np.mean(scores):.7f} (+- {np.std(scores):.7f})\"\n",
    "          f\"#   elapsed time:   {int(np.round(elapsed_time.total_seconds() / 60))} min\")\n",
    "\n",
    "    return oof_preds, test_preds\n",
    "\n",
    "def cross_validate_catboost(X, y, test_data, n_splits=5, n_bags=3):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    test_preds = np.zeros((test_data.shape[0], n_splits))\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(X, y)):\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_val = X.iloc[valid_index]\n",
    "        \n",
    "        # Use standard indexing for y since it's a NumPy array\n",
    "        y_train = y[train_index]\n",
    "        y_val = y[valid_index]\n",
    "        \n",
    "        for i in range(n_bags):\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=5000,\n",
    "                learning_rate=0.03,\n",
    "                depth=6,\n",
    "                eval_metric='AUC',\n",
    "                random_seed=42,\n",
    "                logging_level='Silent'\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100, verbose=100)\n",
    "            \n",
    "            # OOF predictions\n",
    "            oof_preds[valid_index] += model.predict_proba(X_val)[:, 1] / n_bags\n",
    "            \n",
    "            # Test predictions\n",
    "            test_preds[:, fold] += model.predict_proba(test_data)[:, 1] / n_bags\n",
    "    \n",
    "    return oof_preds, test_preds.mean(axis=1)\n",
    "\n",
    "def blend_predictions(weights, oof_preds):\n",
    "    return np.sum([w * pred for w, pred in zip(weights, oof_preds)], axis=0)\n",
    "\n",
    "def evaluate_blend(oof_preds, y, weights):\n",
    "    blended_preds = blend_predictions(weights, oof_preds)\n",
    "    return roc_auc_score(y, blended_preds)\n",
    "\n",
    "def hill_climb_blend(oof_preds, y, max_iterations=100):\n",
    "    # Start with equal weights\n",
    "    num_models = len(oof_preds)\n",
    "    best_weights = np.ones(num_models) / num_models\n",
    "    best_score = evaluate_blend(oof_preds, y, best_weights)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        for i in range(num_models):\n",
    "            # Create new weights by tweaking one weight\n",
    "            new_weights = best_weights.copy()\n",
    "            new_weights[i] += 0.01  # Increase weight for model i\n",
    "            new_weights = new_weights / np.sum(new_weights)  # Normalize weights\n",
    "            \n",
    "            new_score = evaluate_blend(oof_preds, y, new_weights)\n",
    "\n",
    "            if new_score > best_score:\n",
    "                best_score = new_score\n",
    "                best_weights = new_weights\n",
    "\n",
    "            # Try decreasing the weight as well\n",
    "            new_weights = best_weights.copy()\n",
    "            new_weights[i] -= 0.01  # Decrease weight for model i\n",
    "            new_weights = new_weights / np.sum(new_weights)  # Normalize weights\n",
    "            \n",
    "            new_score = evaluate_blend(oof_preds, y, new_weights)\n",
    "\n",
    "            if new_score > best_score:\n",
    "                best_score = new_score\n",
    "                best_weights = new_weights\n",
    "\n",
    "    return best_weights, best_score\n",
    "\n",
    "# Preprocess the training and test data\n",
    "X = preprocess(train_data.drop(columns=['id', 'loan_status']))\n",
    "y = train_data['loan_status'].values\n",
    "test_data = preprocess(test_data.drop(columns=['id']))\n",
    "\n",
    "# Cross-validate models and get OOF predictions\n",
    "oof_xgb, test_xgb = cross_validate(n_splits=5, n_bags=3, model_name='xgb')\n",
    "oof_lgbm, test_lgbm = cross_validate(n_splits=5, n_bags=3, model_name='lgb')\n",
    "oof_cat, test_cat = cross_validate_catboost(X, y, test_data, n_splits=5, n_bags=3)\n",
    "\n",
    "# Combine OOF predictions\n",
    "oof_preds = [oof_cat, oof_xgb, oof_lgbm]\n",
    "\n",
    "# Optimize weights using hill climbing\n",
    "optimal_weights, best_score = hill_climb_blend(oof_preds, y, max_iterations=100)\n",
    "print(\"Optimal weights:\", optimal_weights)\n",
    "print(\"Best ROC-AUC Score:\", best_score)\n",
    "\n",
    "# Generate final predictions using the optimal weights\n",
    "test_preds = [test_cat, test_xgb, test_lgbm]\n",
    "final_predictions = blend_predictions(optimal_weights, test_preds)\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data.index,\n",
    "    'loan_status': final_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('subm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd99f8a-b38c-4029-88b5-a293783edf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 22:21:44,251] A new study created in memory with name: no-name-aeb45f36-cd07-49f5-9a83-3e032ac435ef\n",
      "[I 2024-10-17 22:21:51,820] Trial 0 finished with value: 0.9477904078511632 and parameters: {'n_estimators': 1610, 'learning_rate': 0.04907530236412198, 'max_depth': 10, 'colsample_bytree': 0.5329081859610444, 'subsample': 0.8568878497559596, 'min_child_weight': 9}. Best is trial 0 with value: 0.9477904078511632.\n",
      "[I 2024-10-17 22:22:01,649] Trial 1 finished with value: 0.9515640951916625 and parameters: {'n_estimators': 2013, 'learning_rate': 0.013437528938009294, 'max_depth': 10, 'colsample_bytree': 0.35742230234870503, 'subsample': 0.6881811150449835, 'min_child_weight': 6}. Best is trial 1 with value: 0.9515640951916625.\n",
      "[I 2024-10-17 22:22:04,223] Trial 2 finished with value: 0.9521270849294551 and parameters: {'n_estimators': 1205, 'learning_rate': 0.04017326723949277, 'max_depth': 3, 'colsample_bytree': 0.5233803401607108, 'subsample': 0.7868812640268696, 'min_child_weight': 5}. Best is trial 2 with value: 0.9521270849294551.\n",
      "[I 2024-10-17 22:22:06,729] Trial 3 finished with value: 0.9412362688878133 and parameters: {'n_estimators': 1021, 'learning_rate': 0.012589465489725336, 'max_depth': 3, 'colsample_bytree': 0.991034577676555, 'subsample': 0.9792705383756468, 'min_child_weight': 4}. Best is trial 2 with value: 0.9521270849294551.\n",
      "[I 2024-10-17 22:22:15,761] Trial 4 finished with value: 0.9517095841124193 and parameters: {'n_estimators': 1933, 'learning_rate': 0.010035623555417239, 'max_depth': 9, 'colsample_bytree': 0.6358295052512056, 'subsample': 0.7209128370633353, 'min_child_weight': 10}. Best is trial 2 with value: 0.9521270849294551.\n",
      "[I 2024-10-17 22:22:26,823] Trial 5 finished with value: 0.9471182002234719 and parameters: {'n_estimators': 1920, 'learning_rate': 0.0487270152780365, 'max_depth': 9, 'colsample_bytree': 0.600425604054863, 'subsample': 0.7385771458982893, 'min_child_weight': 1}. Best is trial 2 with value: 0.9521270849294551.\n",
      "[I 2024-10-17 22:22:29,492] Trial 6 finished with value: 0.953030211572084 and parameters: {'n_estimators': 1122, 'learning_rate': 0.07499910416628376, 'max_depth': 4, 'colsample_bytree': 0.36405528872896836, 'subsample': 0.9525207123967265, 'min_child_weight': 5}. Best is trial 6 with value: 0.953030211572084.\n",
      "[I 2024-10-17 22:22:35,597] Trial 7 finished with value: 0.947271654126879 and parameters: {'n_estimators': 1597, 'learning_rate': 0.09577404365760996, 'max_depth': 6, 'colsample_bytree': 0.8914647241217295, 'subsample': 0.976102257498267, 'min_child_weight': 3}. Best is trial 6 with value: 0.953030211572084.\n",
      "[I 2024-10-17 22:22:48,096] Trial 8 finished with value: 0.9484032055185783 and parameters: {'n_estimators': 2669, 'learning_rate': 0.016912454426754, 'max_depth': 7, 'colsample_bytree': 0.9943586413161731, 'subsample': 0.519770423802818, 'min_child_weight': 3}. Best is trial 6 with value: 0.953030211572084.\n",
      "[I 2024-10-17 22:22:56,153] Trial 9 finished with value: 0.9477172466876567 and parameters: {'n_estimators': 1721, 'learning_rate': 0.05702491510866506, 'max_depth': 9, 'colsample_bytree': 0.34470280178704465, 'subsample': 0.835453673728568, 'min_child_weight': 5}. Best is trial 6 with value: 0.953030211572084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'n_estimators': 1122, 'learning_rate': 0.07499910416628376, 'max_depth': 4, 'colsample_bytree': 0.36405528872896836, 'subsample': 0.9525207123967265, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the data\n",
    "test_data = pd.read_csv(\"/Users/raghavgarg/Downloads/playground-series-s4e10/test.csv\")\n",
    "train_data = pd.read_csv(\"/Users/raghavgarg/Downloads/playground-series-s4e10/train.csv\")\n",
    "original_data = pd.read_csv(\"/Users/raghavgarg/Downloads/credit_risk_dataset.csv\")\n",
    "\n",
    "# Preprocessing function (refined)\n",
    "def preprocess(X):\n",
    "    X = X.copy()\n",
    "    # Creating derived features\n",
    "    X['age_income_interaction'] = X['person_age'] * X['person_income']\n",
    "    X['loan_to_emp_length_ratio'] = X['loan_amnt'] / (X['person_emp_length'] + 1)\n",
    "    monthly_income = X['person_income'] / 12\n",
    "    X['monthly_debt'] = X['loan_amnt'] * (1 + X['loan_int_rate']) / 12\n",
    "    X['dti_ratio'] = X['monthly_debt'] / monthly_income\n",
    "    X['risk_flag'] = np.where((X['cb_person_default_on_file'] == 'Y') & (X['loan_grade'].isin(['C', 'D', 'E'])), 1, 0)\n",
    "\n",
    "    # Categorical and numerical columns\n",
    "    categorical_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "    # Feature engineering\n",
    "    X['person_income_to_age'] = X['person_income'] / (X['person_age'] + 1)\n",
    "    X['loan_amnt_to_income_ratio'] = X['loan_amnt'] / (X['person_income'] + 1)\n",
    "    X['emp_length_to_age_ratio'] = X['person_emp_length'] / (X['person_age'] + 1)\n",
    "\n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    cat_col_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols)\n",
    "    all_col_names = numerical_cols + list(cat_col_names)\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=all_col_names)\n",
    "\n",
    "    return X_transformed_df\n",
    "\n",
    "def down_sampling(X, y, i):\n",
    "    \"\"\"Down-sample majority class.\"\"\"\n",
    "    X = pd.DataFrame(X).reset_index(drop=True)  # Convert to DataFrame if not already\n",
    "    y = pd.Series(y).reset_index(drop=True)     # Convert to Series if not already\n",
    "    majority_class = X[y == 0]\n",
    "    minority_class = X[y == 1]\n",
    "    majority_sample = majority_class.sample(len(minority_class), random_state=i)\n",
    "    y_minimal = pd.concat([y.iloc[majority_sample.index], y[y == 1]])\n",
    "    X_minimal = pd.concat([majority_sample, minority_class], axis=0)\n",
    "    return X_minimal, y_minimal\n",
    "\n",
    "\n",
    "# Optuna Hyperparameter Optimization for XGBoost\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10)\n",
    "    }\n",
    "    model = XGBClassifier(**params, random_state=42, n_jobs=4)\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    score = 0\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        X_train, y_train = down_sampling(X_train, y_train, 10)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        score += roc_auc_score(y_val, preds)\n",
    "    return score / 5\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_params = study.best_params\n",
    "print(\"Best XGBoost params:\", best_params)\n",
    "\n",
    "# Models\n",
    "xgb_model = XGBClassifier(**best_params, random_state=42, n_jobs=4)\n",
    "lgbm_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=5000, learning_rate=0.03, depth=6, eval_metric='AUC', random_seed=42, logging_level='Silent')\n",
    "\n",
    "# Stacking\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        ('catboost', catboost_model)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Preprocess and train\n",
    "X = preprocess(train_data.drop(columns=['id', 'loan_status']))\n",
    "y = train_data['loan_status'].values\n",
    "test_data_processed = preprocess(test_data.drop(columns=['id']))\n",
    "\n",
    "stacked_model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "test_preds = stacked_model.predict_proba(test_data_processed)[:, 1]\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'loan_status': test_preds\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission_stacked.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77debfbb-eb94-4575-bd84-f0224bb1b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'person_age', 'person_income', 'person_home_ownership',\n",
       "       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n",
       "       'loan_int_rate', 'loan_percent_income', 'cb_person_default_on_file',\n",
       "       'cb_person_cred_hist_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32e0b0-b87d-46a7-a97c-79aa8a0a5f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
